# Using Local LLMs with Ollama

We can use Ollama to run open-source models locally rather than using closed Anthropic or OpenAI models like GPT-5.


# Dockering Ollama Server

Running it is pretty easy, but if we want production-grade LLM models running, it is a good idea to containerize our Ollama server as a microservice.